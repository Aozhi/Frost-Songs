{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Frost Songs",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/Frost-Songs/blob/main/Frost_Songs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg3sWsS0OkDk"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "print('Copying Salamander piano SoundFont (via https://sites.google.com/site/soundfonts4u) from GCS...')\n",
        "!gsutil -q -m cp -r gs://magentadata/models/music_transformer/primers/* /content/\n",
        "# !gsutil -q -m cp gs://magentadata/soundfonts/Yamaha-C5-Salamander-JNv5.1.sf2 /content/\n",
        "!gsutil -q -m cp gs://download.magenta.tensorflow.org/soundfonts/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2 /content/\n",
        "\n",
        "print('Installing dependencies...')\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 build-essential libasound2-dev libjack-dev\n",
        "!pip install -q 'tensorflow-datasets < 4.0.0'\n",
        "!pip install -qU google-cloud magenta pyfluidsynth\n",
        "\n",
        "import ctypes.util\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return ctypes.util.find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library\n",
        "\n",
        "print('Importing libraries...')\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from tensor2tensor import models\n",
        "from tensor2tensor import problems\n",
        "from tensor2tensor.data_generators import text_encoder\n",
        "from tensor2tensor.utils import decoding\n",
        "from tensor2tensor.utils import trainer_lib\n",
        "\n",
        "from magenta.models.score2perf import score2perf\n",
        "import note_seq\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "SF2_PATH = '/content/SGM-v2.01-Sal-Guit-Bass-V1.3.sf2'\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "# Upload a MIDI file and convert to NoteSequence.\n",
        "def upload_midi():\n",
        "  data = list(files.upload().values())\n",
        "  if len(data) > 1:\n",
        "    print('Multiple files uploaded; using only one.')\n",
        "  return note_seq.midi_to_note_sequence(data[0])\n",
        "\n",
        "# Decode a list of IDs.\n",
        "def decode(ids, encoder):\n",
        "  ids = list(ids)\n",
        "  if text_encoder.EOS_ID in ids:\n",
        "    ids = ids[:ids.index(text_encoder.EOS_ID)]\n",
        "  return encoder.decode(ids)\n",
        "\n",
        "print('Done!')\n",
        "\n",
        "model_name = 'transformer'\n",
        "hparams_set = 'transformer_tpu'\n",
        "ckpt_path = 'gs://magentadata/models/music_transformer/checkpoints/melody_conditioned_model_16.ckpt'\n",
        "\n",
        "class MelodyToPianoPerformanceProblem(score2perf.AbsoluteMelody2PerfProblem):\n",
        "  @property\n",
        "  def add_eos_symbol(self):\n",
        "    return True\n",
        "\n",
        "problem = MelodyToPianoPerformanceProblem()\n",
        "melody_conditioned_encoders = problem.get_feature_encoders()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5ywNt1pOj5D"
      },
      "source": [
        "# Set up HParams.\n",
        "hparams = trainer_lib.create_hparams(hparams_set=hparams_set)\n",
        "trainer_lib.add_problem_hparams(hparams, problem)\n",
        "hparams.num_hidden_layers = 16\n",
        "hparams.sampling_method = 'random'\n",
        "\n",
        "# Set up decoding HParams.\n",
        "decode_hparams = decoding.decode_hparams()\n",
        "decode_hparams.alpha = 0.0 # 0.0\n",
        "decode_hparams.beam_size = 1 # 1\n",
        "\n",
        "# Create Estimator.\n",
        "run_config = trainer_lib.create_run_config(hparams)\n",
        "estimator = trainer_lib.create_estimator(\n",
        "    model_name, hparams, run_config,\n",
        "    decode_hparams=decode_hparams)\n",
        "\n",
        "# These values will be changed by the following cell.\n",
        "inputs = []\n",
        "decode_length = 0\n",
        "\n",
        "# Create input generator.\n",
        "def input_generator():\n",
        "  global inputs\n",
        "  while True:\n",
        "    yield {\n",
        "        'inputs': np.array([[inputs]], dtype=np.int32),\n",
        "        'targets': np.zeros([1, 0], dtype=np.int32),\n",
        "        'decode_length': np.array(decode_length, dtype=np.int32)\n",
        "    }\n",
        "\n",
        "# Start the Estimator, loading from the specified checkpoint.\n",
        "input_fn = decoding.make_input_fn_from_generator(input_generator())\n",
        "melody_conditioned_samples = estimator.predict(\n",
        "    input_fn, checkpoint_path=ckpt_path)\n",
        "\n",
        "# \"Burn\" one.\n",
        "_ = next(melody_conditioned_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pVAK_m2-vf"
      },
      "source": [
        "!fileid=\"1FbKTMX4w7nKyMf4-ZQF5J5DC71S-GPzh\"; filename=\"20200919Updatedyylab1Release1.zip\"; \\\n",
        "curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=${fileid}\" > /dev/null; \\\n",
        "curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=${fileid}\" -o ${filename}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKFo6kw9o-dX"
      },
      "source": [
        "!unzip 20200919Updatedyylab1Release1.zip\n",
        "!mkdir Lyrics-Conditioned-Neural-Melody-Generation\n",
        "!mv 20200919Updatedyylab1Release1/0919/* Lyrics-Conditioned-Neural-Melody-Generation\n",
        "!pip install py-midi\n",
        "!pip install pretty_midi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5kGvOvQX30n"
      },
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "syll_model_path = 'Lyrics-Conditioned-Neural-Melody-Generation/enc_models/syllEncoding_20190419.bin'\n",
        "word_model_path = 'Lyrics-Conditioned-Neural-Melody-Generation/enc_models/wordLevelEncoder_20190419.bin'\n",
        "syllModel = Word2Vec.load(syll_model_path)\n",
        "wordModel = Word2Vec.load(word_model_path)\n",
        "\n",
        "!pip install pyphen\n",
        "\n",
        "import pyphen\n",
        "dic = pyphen.Pyphen(lang='en_US')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyrsNrs6uEKj"
      },
      "source": [
        "import music21\n",
        "def create_music_21_pattern(sample):\n",
        "  notes = []\n",
        "  offset = 0\n",
        "  for i in range(0, len(sample)):\n",
        "    pitch=int(sample[i][0])\n",
        "    n = note.Note(pitch)\n",
        "    length = round(float(sample[i][1]),4)\n",
        "    n.quarterLength = length\n",
        "    n.offset = offset\n",
        "    notes.append(n)\n",
        "    offset += length\n",
        "  return notes\n",
        "\n",
        "def transpose_notes(notes, new_key):\n",
        "  midi_stream = music21.stream.Stream(notes)\n",
        "  key = midi_stream.analyze('key')\n",
        "  print(\"transpose from\", key, \"to\", new_key)\n",
        "  interval = music21.interval.Interval(key.tonic, new_key.tonic)\n",
        "  new_stream = midi_stream.transpose(interval)\n",
        "  return new_stream.notes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TdTOvyXX30y"
      },
      "source": [
        "from music21 import note, stream\n",
        "import re\n",
        "model_path = \"Lyrics-Conditioned-Neural-Melody-Generation/saved_gan_models/saved_model_best_overall_mmd\"\n",
        "\n",
        "def generate_melody(flattened_cond, length_song):\n",
        "  x_list = []\n",
        "  y_list = []\n",
        "\n",
        "  with tf.Session(graph=tf.Graph()) as sess:\n",
        "    tf.saved_model.loader.load(sess, [], model_path)\n",
        "    graph = tf.get_default_graph()\n",
        "    keep_prob = graph.get_tensor_by_name(\"model/keep_prob:0\")\n",
        "    input_metadata = graph.get_tensor_by_name(\"model/input_metadata:0\")\n",
        "    input_songdata = graph.get_tensor_by_name(\"model/input_data:0\")\n",
        "    output_midi = graph.get_tensor_by_name(\"output_midi:0\")\n",
        "    feed_dict = {}\n",
        "    feed_dict[keep_prob.name] = 1.0\n",
        "    condition = []\n",
        "    feed_dict[input_metadata.name] = condition\n",
        "    feed_dict[input_songdata.name] = np.random.uniform(size=(1, 20, 3))\n",
        "    condition.append(np.split(np.asarray(flattened_cond), 20))\n",
        "    feed_dict[input_metadata.name] = condition\n",
        "    generated_features = sess.run(output_midi, feed_dict)\n",
        "    sample = [x[0, :] for x in generated_features]\n",
        "    midi_pattern = create_music_21_pattern(sample[0:length_song])\n",
        "    return(midi_pattern)\n",
        "\n",
        "poem = '''I have wished a bird would fly away,\n",
        "And not sing by my house all day;\n",
        "Have clapped my hands at him from the door\n",
        "When it seemed as if I could bear no more.\n",
        "The fault must partly have been in me.\n",
        "The bird was not to blame for his key.\n",
        "And of course there must be something wrong\n",
        "In wanting to silence any song.'''\n",
        "\n",
        "pre_process = True\n",
        "lines = poem.split('\\n')\n",
        "bpm = 110\n",
        "notes = []\n",
        "song_length = 0\n",
        "the_key = music21.key.Key(\"a\") # A Minor\n",
        "key_of_a_minor = {0, 2, 4, 5, 7, 9, 11}\n",
        "lyric_syllables = []\n",
        "lyric_times = []\n",
        "lyric_pitches = []\n",
        "\n",
        "for line in lines:\n",
        "  line = re.sub(r'[^a-zA-Z ]+', '', line.strip())\n",
        "  print(line)\n",
        "  line = re.sub(' +', ' ', line)\n",
        "  words = line.split(' ')\n",
        "  lyrics = []\n",
        "  for word in words:\n",
        "    syllables = dic.inserted(word).split('-')\n",
        "    if len(syllables) > 0:\n",
        "      for syllable in syllables:\n",
        "        if len(syllable) is 0:\n",
        "          continue\n",
        "        lyric_syllables.append(syllable)\n",
        "        if syllable in syllModel.wv.vocab and word in wordModel.wv.vocab:\n",
        "          # print(syllable)\n",
        "          lyrics.append([syllable, word])\n",
        "        else:\n",
        "          lyrics.append([\"la\", \"la\"])\n",
        "          # print(\"la\")\n",
        "    else:\n",
        "      lyric_syllables.append(word)\n",
        "      if len(syllable) is 0:\n",
        "          continue\n",
        "      if word in wordModel.wv.vocab and syllable in syllModel.wv.vocab:\n",
        "        lyrics.append([word, word])\n",
        "        # print(word)\n",
        "      else:\n",
        "        lyrics.append([\"la\", \"la\"])\n",
        "        # print(\"la\")\n",
        "\n",
        "  length_song = len(lyrics)\n",
        "  cond = []\n",
        " \n",
        "  for i in range(20):\n",
        "    if i < length_song:\n",
        "      syll2Vec = syllModel.wv[lyrics[i][0]]\n",
        "      word2Vec = wordModel.wv[lyrics[i][1]]\n",
        "      cond.append(np.concatenate((syll2Vec, word2Vec)))\n",
        "    else:\n",
        "      cond.append(np.concatenate((syll2Vec, word2Vec)))\n",
        "\n",
        "  flattened_cond = []\n",
        "  for x in cond:\n",
        "    for y in x:\n",
        "      flattened_cond.append(y)\n",
        "\n",
        "  new_notes = []\n",
        "  pattern = generate_melody(flattened_cond, length_song)\n",
        "  pattern_length = 0\n",
        "\n",
        "  # quantize the start times and note durations\n",
        "  for c, n in enumerate(pattern):\n",
        "    if pre_process:\n",
        "      n.offset = int(float(n.offset)*4+0.5) / 4\n",
        "      n.quarterLength = int(float(n.quarterLength)*4+0.5) / 4\n",
        "    n.offset += song_length\n",
        "    # n.pitch.ps += 12\n",
        "    new_notes.append(n)\n",
        "    pattern_length += n.quarterLength\n",
        "  \n",
        "  diff = 0\n",
        "  # stretch the last note out to hold the time\n",
        "  if pre_process:\n",
        "    pattern_length_adjusted = float(pattern_length-0.125)\n",
        "    new_length = 4 * (1 + pattern_length_adjusted//4)\n",
        "    diff = new_length - float(pattern_length)\n",
        "    new_notes[-1].quarterLength += diff\n",
        "\n",
        "    # transpose\n",
        "    new_notes = transpose_notes(new_notes, the_key)\n",
        "\n",
        "  # if any notes are sharp, knock it down into they key of A minor\n",
        "  for n in new_notes:\n",
        "    if pre_process and int(n.pitch.ps) % 12 not in key_of_a_minor:\n",
        "      n.pitch.ps -= 1\n",
        "    notes.append(n)\n",
        "\n",
        "  song_length += pattern_length + diff\n",
        "\n",
        "  for n in new_notes:\n",
        "    lyric_times.append(float(n.offset))\n",
        "    lyric_pitches.append(n.pitch.ps)\n",
        "\n",
        "print(lyric_syllables)\n",
        "midi_stream = music21.stream.Stream(notes)\n",
        "midi_stream = midi_stream.augmentOrDiminish(120.0/bpm)\n",
        "_ = midi_stream.write('midi', \"test.mid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgRVgjOceeqi"
      },
      "source": [
        "melody_ns = note_seq.midi_file_to_sequence_proto(\"test.mid\")\n",
        "\n",
        "melody_instrument = note_seq.infer_melody_for_sequence(melody_ns)\n",
        "notes = [note for note in melody_ns.notes\n",
        "          if note.instrument == melody_instrument]\n",
        "del melody_ns.notes[:]\n",
        "melody_ns.notes.extend(\n",
        "    sorted(notes, key=lambda note: note.start_time))\n",
        "for i in range(len(melody_ns.notes) - 1):\n",
        "  melody_ns.notes[i].end_time = melody_ns.notes[i + 1].start_time\n",
        "inputs = melody_conditioned_encoders['inputs'].encode_note_sequence(\n",
        "    melody_ns)\n",
        "\n",
        "# Play and plot the melody.\n",
        "note_seq.play_sequence(\n",
        "    melody_ns,\n",
        "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
        "note_seq.plot_sequence(melody_ns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA4yeAUOQaA7"
      },
      "source": [
        "melody_ns = note_seq.midi_file_to_sequence_proto(\"test.mid\")\n",
        "\n",
        "melody_instrument = note_seq.infer_melody_for_sequence(melody_ns)\n",
        "notes = [note for note in melody_ns.notes\n",
        "          if note.instrument == melody_instrument]\n",
        "del melody_ns.notes[:]\n",
        "melody_ns.notes.extend(\n",
        "    sorted(notes, key=lambda note: note.start_time))\n",
        "for i in range(len(melody_ns.notes) - 1):\n",
        "  melody_ns.notes[i].end_time = melody_ns.notes[i + 1].start_time\n",
        "inputs = melody_conditioned_encoders['inputs'].encode_note_sequence(\n",
        "    melody_ns)\n",
        "\n",
        "# Play and plot the melody.\n",
        "note_seq.play_sequence(\n",
        "    melody_ns,\n",
        "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)\n",
        "note_seq.plot_sequence(melody_ns)\n",
        "\n",
        "# Generate sample events.\n",
        "decode_length = 4096\n",
        "sample_ids = next(melody_conditioned_samples)['outputs']\n",
        "\n",
        "# Decode to NoteSequence.\n",
        "midi_filename = decode(\n",
        "    sample_ids,\n",
        "    encoder=melody_conditioned_encoders['targets'])\n",
        "accompaniment_ns = note_seq.midi_file_to_note_sequence(midi_filename)\n",
        "\n",
        "def find_closest_note(n, notes):\n",
        "  closest = None\n",
        "  smallest_diff = float(\"inf\")\n",
        "  for x in notes:\n",
        "    if n.pitch == x.pitch:\n",
        "\n",
        "      start_diff = n.start_time - x.start_time\n",
        "      end_diff = (n.start_time + n.end_time) - (x.start_time + x.end_time)\n",
        "      diff = start_diff * start_diff + end_diff * end_diff\n",
        "\n",
        "      # diff = abs(n.start_time - x.start_time)\n",
        "      # diff += abs((n.start_time + n.end_time) - (x.start_time + x.end_time))\n",
        "      if (diff < smallest_diff):\n",
        "        closest = x\n",
        "        smallest_diff = diff\n",
        "  return closest\n",
        "\n",
        "for n in accompaniment_ns.notes:\n",
        "  n.instrument = 1\n",
        "  n.program = 2 # piano\n",
        "  n.velocity += 20\n",
        "\n",
        "original_pitches = []\n",
        "original_times = []\n",
        "lyric_pitches = []\n",
        "lyric_times = []\n",
        "closest_notes = []\n",
        "\n",
        "for n in melody_ns.notes:\n",
        "  original_pitches.append(n.pitch)\n",
        "  original_times.append(n.start_time)\n",
        "  closest_note = find_closest_note(n, accompaniment_ns.notes)\n",
        "  closest_note.instrument = 0\n",
        "  closest_note.program = 26\n",
        "  closest_notes.append(closest_note)\n",
        "  lyric_times.append(closest_note.start_time)\n",
        "  lyric_pitches.append(closest_note.pitch)\n",
        "\n",
        "# map to the closest notes in the original melody\n",
        "for c in range(len(closest_notes)-1):\n",
        "  if (closest_notes[c].end_time > closest_notes[c+1].start_time and\n",
        "      closest_notes[c+1].start_time > closest_notes[c].start_time):\n",
        "    closest_notes[c].end_time = closest_notes[c+1].start_time\n",
        "\n",
        "# hold the last 5 notes to create an ending\n",
        "pitches = []\n",
        "for i in range(1,6):\n",
        "  n = accompaniment_ns.notes[-i]\n",
        "  if n.pitch not in pitches:\n",
        "    n.end_time += 4\n",
        "\n",
        "# Play and plot.\n",
        "note_seq.play_sequence(\n",
        "    accompaniment_ns,\n",
        "    synth=note_seq.fluidsynth, sample_rate=SAMPLE_RATE, sf2_path=SF2_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hb5bdz5fEuRH"
      },
      "source": [
        "import bokeh\n",
        "from bokeh.models import ColumnDataSource, Label, LabelSet, Range1d\n",
        "from bokeh.plotting import figure, output_file, show\n",
        "\n",
        "fig = note_seq.plot_sequence(accompaniment_ns, show_figure = False)\n",
        "fig.width = 1600\n",
        "fig.height = 900\n",
        "fig.toolbar.logo = None\n",
        "# fig.toolbar_location = None\n",
        "source = ColumnDataSource(data=dict(pitch=lyric_pitches, time=lyric_times, words=lyric_syllables))\n",
        "labels = LabelSet(x='time', y='pitch', text='words', level='overlay',\n",
        "              x_offset=0, y_offset=10, source=source, text_font_size='12px', text_font_style=\"bold\")\n",
        "fig.add_layout(labels)\n",
        "show(fig)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}